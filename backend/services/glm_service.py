#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºè°±AI GLMæ¨¡å‹æœåŠ¡

ä½¿ç”¨BaseModelServiceåŸºç±»çš„å®Œæ•´ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•æ­£ç¡®ç»§æ‰¿å’Œå®ç°æ–°æ¨¡å‹
"""

import os
import json
import logging
from typing import List, Dict, Optional
from .base_model_service import BaseModelService

logger = logging.getLogger(__name__)

class GLMService(BaseModelService):
    """æ™ºè°±AI GLMæ¨¡å‹æœåŠ¡"""
    
    def __init__(self):
        super().__init__("æ™ºè°±AI GLM")
        logger.info(f"ğŸ¤– åˆå§‹åŒ–{self.model_name}æœåŠ¡")
    
    def get_api_config(self) -> Dict[str, str]:
        """
        è·å–GLM APIé…ç½®
        
        Returns:
            åŒ…å«api_keyå’Œapi_baseçš„é…ç½®å­—å…¸
        """
        return {
            "api_key": os.environ.get("GLM_API_KEY", ""),
            "api_base": os.environ.get("GLM_API_BASE", "https://open.bigmodel.cn/api/paas/v4")
        }
    
    def build_request_payload(self, message: str, conversation_history: List[Dict] = None) -> Dict:
        """
        æ„å»ºGLM APIè¯·æ±‚è½½è·
        
        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å²
            
        Returns:
            GLM APIæ ¼å¼çš„è¯·æ±‚è½½è·
        """
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        
        # æ·»åŠ å¯¹è¯å†å²
        if conversation_history:
            for msg in conversation_history:
                if msg.get("role") in ["user", "assistant"]:
                    messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
        
        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({
            "role": "user",
            "content": message
        })
        
        # GLM APIç‰¹å®šçš„è¯·æ±‚æ ¼å¼
        payload = {
            "model": "glm-4",  # ä½¿ç”¨GLM-4æ¨¡å‹
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 2000,
            "stream": True,
            "top_p": 0.9,
            "do_sample": True
        }
        
        logger.debug(f"GLMè¯·æ±‚è½½è·: {json.dumps(payload, ensure_ascii=False, indent=2)}")
        return payload
    
    def get_api_endpoint(self, api_base: str) -> str:
        """
        è·å–GLM APIç«¯ç‚¹
        
        Args:
            api_base: APIåŸºç¡€URL
            
        Returns:
            å®Œæ•´çš„APIç«¯ç‚¹URL
        """
        return f"{api_base}/chat/completions"
    
    def build_headers(self, api_key: str) -> Dict[str, str]:
        """
        æ„å»ºGLMç‰¹å®šçš„è¯·æ±‚å¤´
        
        Args:
            api_key: APIå¯†é’¥
            
        Returns:
            GLM APIæ ¼å¼çš„è¯·æ±‚å¤´
        """
        return {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
            "Accept": "text/event-stream"
        }
    
    def process_stream_chunk(self, chunk: str) -> Optional[str]:
        """
        å¤„ç†GLMæµå¼å“åº”å—
        
        Args:
            chunk: åŸå§‹å“åº”å—
            
        Returns:
            å¤„ç†åçš„SSEæ ¼å¼æ•°æ®
        """
        # GLM APIè¿”å›çš„æ˜¯æ ‡å‡†SSEæ ¼å¼
        chunk = chunk.strip()
        
        if not chunk:
            return None
        
        # å¤„ç†GLMçš„SSEæ ¼å¼å“åº”
        if chunk.startswith('data: '):
            data_content = chunk[6:].strip()  # ç§»é™¤ 'data: ' å‰ç¼€
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç»“æŸæ ‡è®°
            if data_content == '[DONE]':
                return "data: [DONE]\n\n"
            
            try:
                # è§£æJSONæ•°æ®
                data = json.loads(data_content)
                
                # æå–GLMå“åº”ä¸­çš„å†…å®¹
                if 'choices' in data and len(data['choices']) > 0:
                    choice = data['choices'][0]
                    if 'delta' in choice and 'content' in choice['delta']:
                        content = choice['delta']['content']
                        if content:
                            # è½¬æ¢ä¸ºæ ‡å‡†SSEæ ¼å¼
                            response_data = {
                                "choices": [
                                    {
                                        "delta": {
                                            "content": content
                                        }
                                    }
                                ]
                            }
                            return f"data: {json.dumps(response_data, ensure_ascii=False)}\n\n"
                
                # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œè¿”å›åŸå§‹æ•°æ®
                return f"data: {json.dumps(data, ensure_ascii=False)}\n\n"
                
            except json.JSONDecodeError as e:
                logger.warning(f"GLM JSONè§£æå¤±è´¥: {str(e)}, åŸå§‹æ•°æ®: {data_content}")
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œå¯èƒ½æ˜¯çº¯æ–‡æœ¬å†…å®¹
                if data_content and data_content != '[DONE]':
                    content_data = {
                        "choices": [
                            {
                                "delta": {
                                    "content": data_content
                                }
                            }
                        ]
                    }
                    return f"data: {json.dumps(content_data, ensure_ascii=False)}\n\n"
        
        return None
    
    def is_available(self) -> bool:
        """
        æ£€æŸ¥GLMæœåŠ¡æ˜¯å¦å¯ç”¨
        
        Returns:
            æœåŠ¡æ˜¯å¦å¯ç”¨
        """
        try:
            config = self.get_api_config()
            return bool(config.get("api_key") and config.get("api_base"))
        except Exception as e:
            logger.error(f"æ£€æŸ¥GLMæœåŠ¡å¯ç”¨æ€§å¤±è´¥: {str(e)}")
            return False

# åˆ›å»ºå…¨å±€GLMæœåŠ¡å®ä¾‹
_glm_service = GLMService()

# å…¼å®¹æ€§å‡½æ•° - ä¿æŒä¸å…¶ä»–æœåŠ¡ä¸€è‡´çš„æ¥å£
async def get_glm_response(message: str, conversation_history: List[Dict] = None) -> str:
    """
    è°ƒç”¨GLM APIè·å–å“åº”ï¼ˆéæµå¼ï¼‰
    
    Args:
        message: ç”¨æˆ·æ¶ˆæ¯
        conversation_history: å¯¹è¯å†å²
        
    Returns:
        GLMæ¨¡å‹çš„å“åº”æ–‡æœ¬
    """
    try:
        return await _glm_service.get_non_stream_response(message, conversation_history)
    except Exception as e:
        logger.error(f"GLMéæµå¼å“åº”å¤±è´¥: {str(e)}")
        raise

async def get_glm_stream_response(message: str, conversation_history: List[Dict] = None):
    """
    è°ƒç”¨GLM APIè·å–æµå¼å“åº”
    
    Args:
        message: ç”¨æˆ·æ¶ˆæ¯
        conversation_history: å¯¹è¯å†å²
        
    Yields:
        GLMæ¨¡å‹çš„æµå¼å“åº”å—
    """
    try:
        async for chunk in _glm_service.get_stream_response(message, conversation_history):
            yield chunk
    except Exception as e:
        logger.error(f"GLMæµå¼å“åº”å¤±è´¥: {str(e)}")
        raise

def get_glm_service() -> GLMService:
    """
    è·å–GLMæœåŠ¡å®ä¾‹
    
    Returns:
        GLMæœåŠ¡å®ä¾‹
    """
    return _glm_service

# æ™ºèƒ½è¡¥å…¨åŠŸèƒ½ï¼ˆå¯é€‰ï¼‰
def get_glm_intelligent_completions(partial_input: str, max_completions: int = 5) -> List[str]:
    """
    ä½¿ç”¨GLMè¿›è¡Œæ™ºèƒ½è¡¥å…¨
    
    Args:
        partial_input: éƒ¨åˆ†è¾“å…¥æ–‡æœ¬
        max_completions: æœ€å¤§è¡¥å…¨æ•°é‡
        
    Returns:
        è¡¥å…¨å»ºè®®åˆ—è¡¨
    """
    try:
        if not _glm_service.is_available():
            logger.warning("GLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•æä¾›æ™ºèƒ½è¡¥å…¨")
            return []
        
        # æ„å»ºè¡¥å…¨æç¤º
        completion_prompt = f"è¯·ä¸ºä»¥ä¸‹ä¸å®Œæ•´çš„æ–‡æœ¬æä¾›{max_completions}ä¸ªå¯èƒ½çš„è¡¥å…¨å»ºè®®ï¼Œæ¯ä¸ªå»ºè®®å•ç‹¬ä¸€è¡Œï¼š\n\n{partial_input}"
        
        # è¿™é‡Œå¯ä»¥è°ƒç”¨GLM APIè¿›è¡Œè¡¥å…¨
        # ä¸ºäº†ç®€åŒ–ç¤ºä¾‹ï¼Œè¿”å›ç©ºåˆ—è¡¨
        # å®é™…å®ç°ä¸­å¯ä»¥è°ƒç”¨get_glm_response
        logger.info(f"GLMæ™ºèƒ½è¡¥å…¨è¯·æ±‚: {partial_input[:30]}...")
        return []
        
    except Exception as e:
        logger.error(f"GLMæ™ºèƒ½è¡¥å…¨å¤±è´¥: {str(e)}")
        return [] 